# SSS - Simple storage service / S3 (Regional) :yellow_circle:
## 1. S3: Intro
- **infinitely scaling**  + highly **durable** + highly **available** 
- Also **foundation service**
  - other services use S3 as an integration 
  - main building blocks of AWS
  
- **bucket-naming convention** 
  - No uppercase, No underscore
  - 3-63 characters long
  - Not an IP
  - Must start with lowercase letter or number
  - Must NOT start with the prefix **xn--**
  - Must NOT end with the suffix **-s3alias**
  
- performance:
  - latency (low) : `100-200 ms` :point_left:
  - throughput : `3500-5500 request/sec/prefix`

- **fact**
  - update and read in || --> replace existing > try read > **always return latest value**.
  - upload object to :
    - S3 directly - slow/internet
    - CF-distribution ==> origin:s3 
      - fast/edge-loc (S3 transfer Acceleration)
---
## 2. S3: Usecase
- **Hybrid-Cloud-storage** 
  - Archival
  - Backup (eg: snapshot : ebs,efs,db )
  - ...
- **hosting**
  - Media,
  - Static website 
  - Application/Software delivery
- **DR** 
  - cross-region-replication, 
  - move data from region/az to another then restore.
- **BigData Analytics**
  - run Data lakes in s3 

---
## 3. S3: Bucket
- s3://my-bucket/prefix-1/`my_file.txt`
  - looks like directory but **no-concept-of-directory**
  
- **bucket type**
  - general-purpose **
  - Directory (New)
  
### 3.1. Object
- max size :`5TB`
- can enable **version + replication**

#### access endpoint
- expose s3 object as public access endpoint
  - attach **access point policy**
  - select object by **prefix**
  - creates **dns** entry

#### access endpoint (with lambda )
- once object is fetch want to run some operation on object using lambda.
- kind of ETL::transform

#### pre-signed URL
- temp access on object
- generated from UI/cli a Pre-signed URL and share it
- can set expiry

- ![img.png](../99_img/storage/snow/img-98.png)

- ![img.png](../99_img/storage/s3-3/img-99.png)   

---
### 3.2. Object: Storage Classes :green_circle:

#### Amazon S3 Standard / frequent
- General Purpose 
- highly durable/available.

#### Infrequent Access (IA) 
- low cost
- usecase - DR-backup/recovery
- min storage duration : 30 days

#### One Zone-Infrequent Access 
- single-AZ,  low ava
- min storage duration : 30 days
- usecase - data which can recreate.

#### Amazon S3 Glacier 
- low cost  
- retrieval mode:
  - **Instant Retrieval** : 
    - in ms
    - min storage duration : 90 days
  - **Flexible Retrieval** 
    - 1-5 min
    - 3-5 hr 
    - 5-12 hr `free`
    - min storage duration : 90 days
  - **Deep Archive** 
    - 12-48 hr `free`
    - min storage duration : 180 days

#### Amazon S3 Intelligent Tiering
- move object b/w tier based on (usage + config)
- mix of above classes into single
- tiers
  - `Frequent Access` tier (automatic): default tier
  - `Infrequent Access` tier (automatic): objects not accessed for 30 days
  - `Archive Instant Access` tier (automatic): objects not accessed for 90 days
  - `Archive Access tier` (optional): configurable from 90 days to 700+ days
  - `Deep-Archive Access` tier (optional): config. from 180 days to 700+ days
- more
  - on bucketObject (standard GP / Standard IA) --> run **s3 analytics** --> take 24/28 hrs --> **CSV report**
  - report gives recommendation 
    - to create life cycle rule to move object b/w storage classes.

#### comparison
- ![img_4.png](../99_img/storage/s3-1/img_4.png)
- ![img.png](../99_img/storage/s3-2/img.png)
- ![img_1.png](../99_img/storage/s3-2/img_1.png)

---
## 4. S3 bucket: versioning :green_circle:
- enable for each bucket + UI:showVersion.
- can roll back to previous version/s
- protect again un-intended delete.
- delete:
  - `delete` : adds delete marker.
  - `delete marker` : undo delete.
  - UI: show-version > select version > `delete the version` : permanent deletes.

---
## 5. S3 bucket: Replication  :green_circle:
- async :point_left:
- create replication `rule`.
  - First, `enable version` is must on src and dest buckets + `attach IAM`
  - replicate object + replicate delete marker(y/n)

- only `new` object will be replicated.
- for `old` objects : use `S3 Batch replication`  separately.
- `No chaining`:
  - having b1 --> b2 | b2 --> b3
  - if obj-1 added in b1, it will be replicated to b2 only.
- Type:SRR (same region) + CRR (Cross region)
- CRR (encryption options )
  - `kms keys (single region)` 
    - un-encrypted object --> Replicated to region-2
    - encrypted object (sse-s3, sse-c) --> Replicated to region-2
    - encrypted object (sse-kms) : `key-region-1`
      - specify the kms-key in another region, which will be used for encryption : `key-region-2`
      - update  key-region-2::`custom policy` for your specific access.
      - or, use key-region-2::`default policy` : access already given to all principle in same account.
      - Re-encrypt with key-region-2  
      - finally, replicated to region-2
     
  - `kms keys (multi-region) `
    - un-encrypted object --> Replicated to region-2 : same as abv
    - encrypted object (sse-s3, sse-c) --> Replicated to region-2  : same as abv
    - encrypted object (sse-kms) : `key-1`
      - same key-1 is present in region-1 and region-2
      - but s3 will treat then as diff keys 
      - and, perform: `decryption and rn-encryption` again [IMP]

---
## 6. S3: Job/batch :yellow_circle:
- Perform **bulk operations** on **existing** S3 objects with a single request.
  - Modify object/s metadata, properties, tags, etc
  - manual object/s replication :point_left:
  - Encrypt un-encrypted objects
  - Modify ACLs
  - Restore objects from S3 Glacier
  - Invoke Lambda function to perform custom action on each object
- **S3:job**
  - Define list of Objects
  - action
  - optional param
-  Flow
- `s3:Inventory` > `s3:Select` > `S3:Job`  > process each object

---
## 7. S3: Event Notification :yellow_circle:
- sends event to **target** :
  - s3:event --> `sns`
  - s3:event --> `Lambda` 
  - s3:event --> `SQS`
  - ...
  - s3:event --> EventBridge (filtering) --> target(18+ services)  
    - archive event
    - replay event

---
## 8. S3: Security
- At account level can enable/disable **public flag**

### 1. encryption
- sse-s3
- sse-kms
- sse-c

### 2. IAM policy
#### 2.1  User-Based IAM polices
- eg:
  - lambda role-1 - add permission for s3:*
  - IAM user - add permission for s3:*

- ![img_3.png](../99_img/storage/s3-3/img_3.png)
- ![img_2.png](../99_img/storage/s3-3/img_2.png)

#### 2.2 Resource-Based
- **S3 Bucket Policies**
  - bucket wide rules : effect,principal,Action,resource
  - eg:
    - Allow public access
    - allows cross account
    - Force objects to be encrypted at upload
      - allow/deny : putObject on bucket-1,  `condition : sse-kms=true`
      - allow/deny : putObject on bucket-1,  `condition: secureTransport=true`
    - ...
    - ![img_1.png](../99_img/storage/s3-3/img_1.png)

- **Object ACL**

- **Bucket ACL**

---
## 9. S3: More
### 9.1 transfer-acceleration (UP)
- upload in **multipart** if 100MB+,  recommended for 5GB+
- uses edge-locations

### 9.2 byte-range fetch (DOWN)
- parallelize GETS to speed up download. can configure how many `bytes` to read.
- ![img_3.png](../99_img/storage/s3-2/img_3.png)
- ![img_4.png](../99_img/storage/s3-2/img_4.png)
  
### 9.3 server side filtering
- use **S3-select** + **Glacier-Select**
- filters csv row and column, more like SQL queries. 
- ![img_5.png](../99_img/storage/s3-2/img_5.png)

### 9.4 requester pay
- network cost on requester
- ![img_2.png](../99_img/storage/s3-2/img_2.png)

---
## 99. S3: hands on
```  
  - create bucket - bucket-1-us-west-2, will be created in all AZ.
  - disable : ACL, , versioning
  - enable : public access + attach BUCKET POLICIES (read from any principle, resource:*)
  - encryption: SSE-S3 *, SSE-KMS, DSSE-KMS
  - upload png file
  - https://bucket-1-us-west-2.s3.us-west-2.amazonaws.com/Screenshot+2024-07-16+002401.png
  - inspect : 
      - Open link --> `s3 pre-signed url`, credential-info are encode in url.
      - access with public-url :  failed | ok after making public.
  - static website hoisting : enable on bucket + index.html
    - endpoints: https://bucket-1.s3-website.region.amazon.com
    
  // Replication
  - create another  buvket-2-us-west-1
  - create replication rule:
    - add target bucket : bucket-1-us-west-2
    - enable versionsing
    - craete new IAM
    
  // storage classes:
  - bucket-1 > mgt > create Life Cycle rule
    - select object: ALL /  by-prefix/suffix / by-tag
    - create transistion Rule/s
      - rule-1 : move from class1 to clas2 after xxdays
      - ...
      - ... 
    - -create deletion Rule/s
      - delete old version/s
      - delete all version/s
      - mark for delete
      - delete incomplete multi-part
```
---
# S3 Glacier object : Vault lock :yellow_circle:
- WORM policy : write once, read many.
- set `retention period`, can be extend.
- `usecase`: data retention. and compliance
- `lock` object in glacier
- retention mode: 
  - `compliance` --> no longer be deleted/updated in the future, not even by root.


## S3 object : lock
- WORM policy.
- set `retention period`, can be extend.
- set (optional) `legal hold` : lock `indefinitely`. (irrespective of retention-period)
- retention mode: 
  - `compliance` --> no longer be deleted/updated in the future, not even by root.
  - `Governance` --> root user can update/delete.

---
# Storage lens service  :yellow_circle:
- Understand, analyze, and `optimize` storage across entire `AWS Organization` (acct > region > bucket)
- `dashboard` : enable by default/cant delete.
  - aggregated reports/csv gernerted by specific metric --> can publish to CW for free.
  - `advance` metric (available for 15 month), paid
  - `free` metric (available for 14 day, once generated)
  - metric/s :
    - `summary` metric : insight to object -size, count, fastest growing bucket, etc
    - `cost-optimization` metric : insight to non-current, incomplete multiparts, etc
    - `Data protection` metric: count of encrypted Bucket, replication rule
    - `Access-mgt` : object owner
    - `event` metric : s3-eventNotification count, etc
    - `Activity` + `statusCode` : GET, POST, etc +   count of 200, 404, etc
    - `performance` : s3 transfer acce enable count
    
- ![img_6.png](../99_img/storage/s3-2/img_6.png)

  



